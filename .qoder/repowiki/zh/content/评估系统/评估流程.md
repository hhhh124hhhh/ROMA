# 评估流程

<cite>
**本文档中引用的文件**   
- [evaluation.py](file://evals/evaluation.py)
- [multiprocessing_helper.py](file://evals/multiprocessing_helper.py)
</cite>

## 目录
1. [简介](#简介)
2. [端到端评估流程](#端到端评估流程)
3. [并行化支持机制](#并行化支持机制)
4. [评估任务配置与启动](#评估任务配置与启动)
5. [在SEAL-0和FRAMES数据集上的应用](#在seal-0和frames数据集上的应用)
6. [自定义评估场景扩展](#自定义评估场景扩展)

## 简介
本文档详细说明了`evaluation.py`中实现的端到端评估流程，包括测试用例加载、代理环境初始化、任务执行及运行时数据收集。同时解释了`multiprocessing_helper.py`如何通过并行化提升大规模基准测试效率。结合代码示例展示参数设置、超时控制与异常处理机制，并描述该流程在SEAL-0和FRAMES数据集上的具体应用方式，指导用户扩展自定义评估场景。

## 端到端评估流程

`evaluation.py`实现了完整的端到端评估流程，从加载测试用例到执行任务并收集结果。该流程通过与Sentient Research Agent服务器通信来创建和管理项目，确保评估过程的可重复性和一致性。

评估流程首先通过命令行参数解析输入输出路径、查询列名、样本数量等配置信息。随后连接到本地运行的服务器（默认地址为`http://localhost:5000`），验证服务可用性后加载指定的数据集文件（支持CSV或Parquet格式）。

系统支持灵活的批处理模式，可通过`--start-idx`和`--end-idx`参数指定处理范围，也可使用`--num-examples`限制样本数量。为防止意外中断导致工作丢失，评估器实现了检查点机制，定期将中间结果保存至指定文件，支持断点续评。

核心执行逻辑由`evaluation_worker`函数驱动，每个工作进程独立创建`ServerEvaluationClient`实例，通过HTTP API与服务器交互。对于每个待评估的查询，客户端调用`create_configured_project`方法创建新项目，传入目标查询、代理配置和最大步数等参数。

项目创建后，工作进程持续轮询项目状态直至完成或失败，最长等待时间由`execution-timeout`参数控制。一旦项目结束，系统从返回的结果中提取最终输出文本，优先选择带有引用的内容，其次为普通输出摘要。整个过程中捕获的执行时间、节点数量和完成状态等元数据均被记录下来用于后续分析。

**Section sources**
- [evaluation.py](file://evals/evaluation.py#L297-L618)

## 并行化支持机制

`multiprocessing_helper.py`模块提供了对大规模基准测试的并行化支持，显著提升了评估效率。该模块利用Python的`multiprocessing`库实现真正的并行计算，避免了全局解释器锁（GIL）带来的性能瓶颈。

并行化的核心是`run_agent_execution`函数，作为每个子进程的目标函数运行。该函数在独立的内存空间中创建专用的`ProfiledSentientAgent`实例，确保所有状态（包括代理注册表）完全隔离。这种设计允许多个评估任务同时进行而不会相互干扰。

主控程序根据`--num-processes`参数启动指定数量的工作进程，默认值为CPU核心数的最小值与2之间的较小者，以避免速率限制问题。任务队列采用`multiprocessing.Queue`实现，保证线程安全的任务分发和结果收集。

每个工作进程在执行前会引入预设延迟（由`--request-delay`参数控制），以遵守API的速率限制策略。当所有任务分配完毕后，主进程开始从结果队列中收集输出，使用带锁的列表合并来自不同进程的结果，并按原始索引排序以保持顺序一致性。

此并行架构不仅提高了吞吐量，还增强了系统的容错能力——单个进程的崩溃不会影响其他正在进行的评估任务。最终结果汇总后统一写入输出文件，并附带评估时间戳和服务器URL等元信息以便追溯。

**Section sources**
- [multiprocessing_helper.py](file://evals/multiprocessing_helper.py#L5-L44)

## 评估任务配置与启动

评估任务的配置与启动通过`evaluation.py`中的`main()`函数实现，提供了一系列命令行参数用于精细化控制评估行为。用户可以通过这些参数灵活调整评估环境，满足不同场景的需求。

关键配置参数包括：
- `--input-file`: 指定包含测试用例的输入文件路径，默认为`datasets/seal-0.csv`
- `--output-file`: 设置结果输出文件路径，默认为`server_eval_results.csv`
- `--query-column`: 定义包含查询语句的列名，默认为`question`
- `--profile-name`: 选择要使用的代理配置文件，默认为`general_agent`

高级控制选项允许用户启用WebSocket监控（`--enable-websocket`）、开启人机协作模式（`--enable-hitl`）以及调节并发任务数（`--max-concurrent-tasks`）。此外，还可以设置规划深度（`--max-planning-depth`）、最大执行步数（`--max-steps`）和执行超时时间（`--execution-timeout`）等关键性能参数。

异常处理机制贯穿整个评估流程。在项目创建阶段，若HTTP请求失败或响应状态码非201，则标记为错误并继续下一个任务。在等待项目完成期间，任何网络异常都会触发重试逻辑，最多等待指定超时时间后强制获取当前结果。节点结果提取过程包含多层防御性编程，能够妥善处理JSON解析错误、类型不匹配等问题。

为了便于调试和监控，系统在控制台输出丰富的进度信息，包括项目创建成功提示、状态变更通知、执行耗时统计等。完成后的总结报告还会列出前五个项目的ID供前端界面查看，方便用户深入探究特定案例。

**Section sources**
- [evaluation.py](file://evals/evaluation.py#L297-L618)

## 在SEAL-0和FRAMES数据集上的应用

该评估流程特别适用于SEAL-0和FRAMES这两个标准数据集的性能评测。SEAL-0数据集通常存储在`datasets/seal-0.csv`路径下，其结构包含`question`和`answer`两列，分别对应需要解决的研究问题和预期答案。

当针对SEAL-0数据集运行评估时，系统自动加载该文件作为默认输入源。通过`--query-column "question"`参数指定查询字段，代理将尝试生成回答并与真实答案进行比较。评估完成后，可使用配套的`grade_answers_simple.py`脚本对结果进行评分，计算准确率指标。

对于FRAMES数据集，虽然默认路径未明确指定，但可通过`--input-file`参数指向相应的CSV文件。该数据集可能包含更复杂的多跳推理任务，因此建议适当增加`--max-steps`和`--execution-timeout`值以适应更长的执行周期。

两种数据集的应用都受益于评估系统的批处理能力和检查点功能。大型数据集可以分割成多个批次依次处理，每个批次的结果独立保存，既降低了内存压力又提高了可靠性。此外，通过修改`worker_config`中的代理配置，可以在同一数据集上快速切换不同的代理策略进行对比实验。

值得注意的是，评估结果不仅包含最终答案文本，还包括执行时间、生成节点数等有价值的性能指标，这些数据可用于深入分析代理的行为模式和效率特征。

**Section sources**
- [evaluation.py](file://evals/evaluation.py#L297-L618)

## 自定义评估场景扩展

用户可以根据特定需求轻松扩展自定义评估场景。最直接的方式是创建新的输入数据文件，遵循CSV格式并包含必要的查询列。然后通过命令行参数指定该文件路径和其他相关配置即可启动评估。

要定制代理行为，可在`worker_config`字典中修改`agent_config`部分。例如，更改`llm.model`字段以测试不同语言模型的表现，或调整`execution.max_concurrent_nodes`来探索并发度对性能的影响。还可以通过`cache.ttl_seconds`和`cache.max_size`参数研究缓存策略的有效性。

对于需要特殊处理逻辑的场景，可以继承`ServerEvaluationClient`类并重写相应方法。比如，在`wait_for_project_completion`中添加自定义的状态判断规则，或在`get_project_results`里实现更复杂的结果提取算法。

并行化框架也支持进一步优化。用户可根据实际硬件条件调整`--num-processes`参数，在资源充足的情况下提高并发度以缩短总评估时间。同时，合理设置`--request-delay`能有效规避API速率限制，平衡速度与稳定性。

最后，评估结果的后处理同样具有高度可扩展性。除了内置的保存功能外，用户可编写插件读取输出CSV文件，执行额外的分析如聚类、可视化或与其他基线系统比较，从而获得更全面的洞察。

**Section sources**
- [evaluation.py](file://evals/evaluation.py#L152-L294)
- [evaluation.py](file://evals/evaluation.py#L297-L618)