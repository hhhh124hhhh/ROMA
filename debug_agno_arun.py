#!/usr/bin/env python3
"""
ä¸“é—¨è°ƒè¯•agnoåº“çš„arunæ–¹æ³•å¦‚ä½•å¤„ç†æ¶ˆæ¯æ ¼å¼
åˆ†æä¸ºä»€ä¹ˆä¼šå‡ºç°"è§’è‰²ä¿¡æ¯ä¸æ­£ç¡®"é”™è¯¯
"""

import os
import sys
import asyncio
import json
from pathlib import Path
from dotenv import load_dotenv

# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
os.chdir(project_root)

# åŠ è½½ç¯å¢ƒå˜é‡
env_path = project_root / ".env"
if env_path.exists():
    load_dotenv(env_path)

async def debug_agno_arun():
    """è°ƒè¯•agnoåº“çš„arunæ–¹æ³•çš„æ¶ˆæ¯æ ¼å¼å¤„ç†"""
    api_key = os.getenv("ZHIPUAI_API_KEY")
    if not api_key:
        print("âŒ ZHIPUAI_API_KEY æœªé…ç½®")
        return
    
    print(f"âœ… APIå¯†é’¥å·²é…ç½®: {api_key[:10]}...")
    
    try:
        from agno.models.openai import OpenAIChat
        print("âœ… æˆåŠŸå¯¼å…¥agno.models.openai.OpenAIChat")
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥agno: {e}")
        return
    
    # åˆ›å»ºæœ€ç®€å•çš„OpenAIChatå®ä¾‹
    print("\nğŸ§ª åˆ›å»ºç®€å•çš„OpenAIChatå®ä¾‹...")
    try:
        model = OpenAIChat(
            id="glm-4.5",
            base_url="https://open.bigmodel.cn/api/paas/v4/",
            api_key=api_key,
            temperature=0.7,
            max_tokens=200
        )
        print("âœ… æˆåŠŸåˆ›å»ºOpenAIChatå®ä¾‹")
        
        # æ£€æŸ¥å®ä¾‹çš„å±æ€§
        print(f"æ¨¡å‹ID: {model.id}")
        print(f"Base URL: {model.base_url}")
        print(f"System message: {getattr(model, 'system_message', 'None')}")
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºOpenAIChatå®ä¾‹å¤±è´¥: {e}")
        return
    
    # æµ‹è¯•1: æœ€ç®€å•çš„arunè°ƒç”¨
    print("\nğŸ§ª æµ‹è¯•1: æœ€ç®€å•çš„arunè°ƒç”¨")
    try:
    # ç»§ç»­æµ‹è¯•aresponseæ–¹æ³•
    print("\nğŸ§ª æµ‹è¯•: æ£€æŸ¥aresponseæ–¹æ³•")
    try:
        if hasattr(model, 'aresponse'):
            print("âœ… æ‰¾åˆ°aresponseæ–¹æ³•")
            
            # æµ‹è¯•aresponseè°ƒç”¨
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ã€‚"},
                {"role": "user", "content": "è¯·ç®€å•å›å¤ã€‚"}
            ]
            
            response = await model.aresponse(messages=messages)
            print(f"âœ… aresponseæˆåŠŸ: {response.content[:100]}...")
        else:
            print("âŒ æ²¡æœ‰aresponseæ–¹æ³•")
    except Exception as e:
        print(f"âŒ aresponseå¤±è´¥: {e}")
        print(f"âœ… ç®€å•arunæˆåŠŸ: {response.content[:100]}...")
    except Exception as e:
        print(f"âŒ ç®€å•arunå¤±è´¥: {e}")
        print(f"é”™è¯¯ç±»å‹: {type(e)}")
        
        # æ›´è¯¦ç»†çš„é”™è¯¯åˆ†æ
        if hasattr(e, 'response'):
            print(f"HTTPå“åº”: {e.response}")
        if hasattr(e, 'body'):
            print(f"é”™è¯¯ä½“: {e.body}")
    
    # æµ‹è¯•2: å¸¦system_messageçš„OpenAIChat
    print("\nğŸ§ª æµ‹è¯•2: å¸¦system_messageçš„OpenAIChat")
    try:
        model_with_system = OpenAIChat(
            id="glm-4.5",
            base_url="https://open.bigmodel.cn/api/paas/v4/",
            api_key=api_key,
            temperature=0.7,
            max_tokens=200
        )
        print("âœ… æˆåŠŸåˆ›å»ºå¸¦system_messageçš„OpenAIChatå®ä¾‹")
        
        if hasattr(model_with_system, 'aresponse'):
            response = await model_with_system.aresponse(messages=[
                {"role": "user", "content": "è¯·ç®€å•å›å¤"}
            ])
            if response and hasattr(response, 'content'):
                print(f"âœ… å¸¦system_messageçš„aresponseæˆåŠŸ: {response.content[:100]}...")
            else:
                print(f"âœ… å¸¦system_messageçš„aresponseæˆåŠŸ: {response}")
        else:
            print("âŒ æ²¡æœ‰aresponseæ–¹æ³•")
        print(f"âœ… å¸¦system_messageçš„arunæˆåŠŸ: {response.content[:100]}...")
    except Exception as e:
        print(f"âŒ å¸¦system_messageçš„arunå¤±è´¥: {e}")
    
    # æµ‹è¯•3: æ£€æŸ¥OpenAIChatçš„å†…éƒ¨å®ç°
    print("\nğŸ§ª æµ‹è¯•3: æ£€æŸ¥OpenAIChatçš„å†…éƒ¨æ–¹æ³•")
    try:
        # å°è¯•æŸ¥çœ‹OpenAIChatå¦‚ä½•æ„å»ºæ¶ˆæ¯
        print(f"OpenAIChatå¯ç”¨æ–¹æ³•: {[m for m in dir(model) if not m.startswith('_')]}")
        
        # æŸ¥çœ‹æ˜¯å¦æœ‰ä»»ä½•å†…éƒ¨æ–¹æ³•ç”¨äºæ¶ˆæ¯æ„å»º
        internal_methods = [m for m in dir(model) if 'message' in m.lower() or 'prompt' in m.lower()]
        print(f"ä¸æ¶ˆæ¯ç›¸å…³çš„æ–¹æ³•: {internal_methods}")
        
    except Exception as e:
        print(f"âŒ æ£€æŸ¥å†…éƒ¨æ–¹æ³•å¤±è´¥: {e}")
    
    # æµ‹è¯•4: ä½¿ç”¨AgentåŒ…è£…å™¨
    print("\nğŸ§ª æµ‹è¯•4: ä½¿ç”¨AgnoAgentåŒ…è£…å™¨")
    try:
        from agno.agent import Agent
        
        # åˆ›å»ºAgnoAgentå®ä¾‹
        agent = Agent(
            model=OpenAIChat(
                id="glm-4.5", 
                base_url="https://open.bigmodel.cn/api/paas/v4/",
                api_key=api_key,
                temperature=0.7,
                max_tokens=200
            ),
            system_message="ä½ æ˜¯ä¸€ä¸ªè§„åˆ’åŠ©æ‰‹ã€‚",
            name="TestAgent"
        )
        print("âœ… æˆåŠŸåˆ›å»ºAgnoAgent")
        
        # æµ‹è¯•agent.arun
        response = await agent.arun("åˆ¶å®šä¸€ä¸ªç®€å•è®¡åˆ’")
        print(f"âœ… AgnoAgent.arunæˆåŠŸ: {response.content[:100]}...")
        
    except Exception as e:
        print(f"âŒ AgnoAgentæµ‹è¯•å¤±è´¥: {e}")
        print(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
        
        # å°è¯•æ•è·æ›´å¤šé”™è¯¯ä¿¡æ¯
        import traceback
        print(f"é”™è¯¯è¿½è¸ª: {traceback.format_exc()}")

if __name__ == "__main__":
    asyncio.run(debug_agno_arun())