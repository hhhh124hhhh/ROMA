#!/usr/bin/env python3
"""
æ™ºè°±AI OpenAIå…¼å®¹APIæµ‹è¯•è„šæœ¬
"""

import os
import sys
import asyncio
from pathlib import Path
from dotenv import load_dotenv

# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
os.chdir(project_root)

# åŠ è½½ç¯å¢ƒå˜é‡
env_path = project_root / ".env"
if env_path.exists():
    load_dotenv(env_path)
    print(f"âœ… å·²åŠ è½½ç¯å¢ƒé…ç½®æ–‡ä»¶: {env_path.absolute()}")
else:
    print(f"âš ï¸  ç¯å¢ƒé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {env_path.absolute()}")

async def test_zhipuai_openai_compat():
    """æµ‹è¯•æ™ºè°±AI OpenAIå…¼å®¹API"""
    api_key = os.getenv("ZHIPUAI_API_KEY")
    if not api_key or api_key == "your_zhipuai_key_here":
        print("âŒ ZHIPUAI_API_KEY æœªé…ç½®")
        return False
    
    print(f"âœ… APIå¯†é’¥å·²é…ç½®: {api_key[:10]}...")
    
    try:
        from openai import AsyncOpenAI
        
        # åˆ›å»ºOpenAIå®¢æˆ·ç«¯ï¼Œä½¿ç”¨æ™ºè°±AIçš„å…¼å®¹ç«¯ç‚¹
        client = AsyncOpenAI(
            api_key=api_key,
            base_url="https://open.bigmodel.cn/api/paas/v4/"
        )
        
        print("ğŸ§ª æµ‹è¯•1: åŸºç¡€æ¶ˆæ¯")
        try:
            response = await client.chat.completions.create(
                model="glm-4.5",
                messages=[
                    {"role": "user", "content": "ä½ å¥½"}
                ],
                temperature=0.7,
                max_tokens=100
            )
            print(f"âœ… åŸºç¡€æµ‹è¯•æˆåŠŸ: {response.choices[0].message.content}")
        except Exception as e:
            print(f"âŒ åŸºç¡€æµ‹è¯•å¤±è´¥: {e}")
            return False
        
        print("\nğŸ§ª æµ‹è¯•2: å¸¦ç³»ç»Ÿæ¶ˆæ¯")
        try:
            response = await client.chat.completions.create(
                model="glm-4.5",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": "ç”¨ä¸€å¥è¯ç®€å•ä»‹ç»äººå·¥æ™ºèƒ½"}
                ],
                temperature=0.7,
                max_tokens=200
            )
            print(f"âœ… ç³»ç»Ÿæ¶ˆæ¯æµ‹è¯•æˆåŠŸ: {response.choices[0].message.content}")
        except Exception as e:
            print(f"âŒ ç³»ç»Ÿæ¶ˆæ¯æµ‹è¯•å¤±è´¥: {e}")
            return False
        
        print("\nğŸ§ª æµ‹è¯•3: é•¿ç³»ç»Ÿæ¶ˆæ¯ï¼ˆæ¨¡æ‹Ÿagent promptï¼‰")
        long_system_prompt = """You are an expert parallel search decomposition agent specialized in breaking down complex research goals into independent, self-contained search tasks that can execute simultaneously. Your primary role is to create **2 to 4 completely independent search subtasks** that together gather comprehensive information from different sources, domains, or perspectives without any dependencies between them.

**TEMPORAL AWARENESS:**
- Today's date: September 24, 2025
- Your SEARCH capabilities provide access to real-time information and current data
- When planning searches, emphasize gathering the most current and up-to-date information available

**CRITICAL PRINCIPLE: INDEPENDENT SEARCH EXECUTION**
Each search subtask will be executed by an independent agent that has NO KNOWLEDGE of:
- Other search tasks in your plan
- The overall search strategy
- System execution flow
- What other search agents are finding

Therefore, each search subtask MUST be:
- **Self-contained**: Include all necessary context and search parameters
- **Independently executable**: Require no outputs from other search tasks
- **Source-specific**: Focus on different information sources, domains, or perspectives"""

        try:
            response = await client.chat.completions.create(
                model="glm-4.5",
                messages=[
                    {"role": "system", "content": long_system_prompt},
                    {"role": "user", "content": "ç”¨ä¸€å¥è¯ç®€å•ä»‹ç»äººå·¥æ™ºèƒ½"}
                ],
                temperature=0.7,
                max_tokens=200
            )
            print(f"âœ… é•¿ç³»ç»Ÿæ¶ˆæ¯æµ‹è¯•æˆåŠŸ: {response.choices[0].message.content}")
        except Exception as e:
            print(f"âŒ é•¿ç³»ç»Ÿæ¶ˆæ¯æµ‹è¯•å¤±è´¥: {e}")
            return False
        
        print("\nğŸ§ª æµ‹è¯•4: ä½¿ç”¨agno.models.openai.OpenAIChat")
        try:
            from agno.models.openai import OpenAIChat
            
            # åˆ›å»ºOpenAIChatå®ä¾‹
            model = OpenAIChat(
                id="glm-4.5",
                base_url="https://open.bigmodel.cn/api/paas/v4/",
                api_key=api_key,
                temperature=0.7,
                max_tokens=200
            )
            
            response = await model.arun("ç”¨ä¸€å¥è¯ç®€å•ä»‹ç»äººå·¥æ™ºèƒ½")
            print(f"âœ… AgnoOpenAIChatæµ‹è¯•æˆåŠŸ: {response.content}")
        except Exception as e:
            print(f"âŒ AgnoOpenAIChatæµ‹è¯•å¤±è´¥: {e}")
            return False
        
        return True
        
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–åº“: {e}")
        return False
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    success = asyncio.run(test_zhipuai_openai_compat())
    if success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ™ºè°±AI OpenAIå…¼å®¹APIå·¥ä½œæ­£å¸¸")
    else:
        print("\nğŸ’¥ æµ‹è¯•å¤±è´¥ï¼éœ€è¦æ£€æŸ¥é…ç½®æˆ–APIå…¼å®¹æ€§")