#!/usr/bin/env python3
"""
æµ‹è¯•ç®€åŒ–çš„ç³»ç»Ÿæ¶ˆæ¯æ˜¯å¦èƒ½è§£å†³æ™ºè°±AIçš„"è§’è‰²ä¿¡æ¯ä¸æ­£ç¡®"é—®é¢˜
"""

import os
import sys
import asyncio
from pathlib import Path
from dotenv import load_dotenv

# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
os.chdir(project_root)

# åŠ è½½ç¯å¢ƒå˜é‡
env_path = project_root / ".env"
if env_path.exists():
    load_dotenv(env_path)

async def test_simplified_system_message():
    """æµ‹è¯•ç®€åŒ–çš„ç³»ç»Ÿæ¶ˆæ¯"""
    api_key = os.getenv("ZHIPUAI_API_KEY")
    if not api_key:
        print("âŒ ZHIPUAI_API_KEY æœªé…ç½®")
        return
    
    print(f"âœ… APIå¯†é’¥å·²é…ç½®: {api_key[:10]}...")
    
    try:
        from openai import AsyncOpenAI
        
        client = AsyncOpenAI(
            api_key=api_key,
            base_url="https://open.bigmodel.cn/api/paas/v4/"
        )
        
        # æµ‹è¯•1: éå¸¸ç®€çŸ­çš„ç³»ç»Ÿæ¶ˆæ¯
        print("\nğŸ§ª æµ‹è¯•1: ç®€çŸ­ç³»ç»Ÿæ¶ˆæ¯")
        simple_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§„åˆ’åŠ©æ‰‹ã€‚"
        try:
            response = await client.chat.completions.create(
                model="glm-4.5",
                messages=[
                    {"role": "system", "content": simple_prompt},
                    {"role": "user", "content": "è¯·ä¸ºæ™ºè°±AIæŠ€æœ¯ç ”ç©¶åˆ¶å®šä¸€ä¸ªç®€å•è®¡åˆ’"}
                ],
                temperature=0.7,
                max_tokens=500
            )
            print(f"âœ… æˆåŠŸ (é•¿åº¦: {len(simple_prompt)}): {response.choices[0].message.content[:100]}...")
        except Exception as e:
            print(f"âŒ å¤±è´¥ (é•¿åº¦: {len(simple_prompt)}): {e}")
        
        # æµ‹è¯•2: ä¸­ç­‰é•¿åº¦çš„ç³»ç»Ÿæ¶ˆæ¯
        print("\nğŸ§ª æµ‹è¯•2: ä¸­ç­‰é•¿åº¦ç³»ç»Ÿæ¶ˆæ¯")
        medium_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»»åŠ¡è§„åˆ’ä»£ç†ï¼Œä¸“é—¨å°†å¤æ‚ç›®æ ‡åˆ†è§£ä¸ºç‹¬ç«‹çš„å­ä»»åŠ¡ã€‚ä½ çš„ä½œç”¨æ˜¯åˆ›å»º2åˆ°4ä¸ªå®Œå…¨ç‹¬ç«‹çš„å­ä»»åŠ¡ï¼Œè¿™äº›å­ä»»åŠ¡å¯ä»¥åŒæ—¶æ‰§è¡Œï¼Œå…±åŒæ”¶é›†æ¥è‡ªä¸åŒæ¥æºæˆ–è§’åº¦çš„ç»¼åˆä¿¡æ¯ã€‚

é‡è¦åŸåˆ™ï¼š
- æ¯ä¸ªå­ä»»åŠ¡å¿…é¡»æ˜¯è‡ªåŒ…å«çš„
- å­ä»»åŠ¡ä¹‹é—´ä¸èƒ½æœ‰ä¾èµ–å…³ç³»  
- ä¸“æ³¨äºä¸åŒçš„ä¿¡æ¯æ¥æºæˆ–è§’åº¦

è¯·ä»¥JSONæ•°ç»„æ ¼å¼å›å¤ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å«goalã€task_typeå’Œdepends_on_indiceså­—æ®µã€‚"""
        
        try:
            response = await client.chat.completions.create(
                model="glm-4.5",
                messages=[
                    {"role": "system", "content": medium_prompt},
                    {"role": "user", "content": "è¯·ä¸ºæ™ºè°±AIæŠ€æœ¯ç ”ç©¶åˆ¶å®šä¸€ä¸ªè®¡åˆ’"}
                ],
                temperature=0.7,
                max_tokens=500
            )
            print(f"âœ… æˆåŠŸ (é•¿åº¦: {len(medium_prompt)}): {response.choices[0].message.content[:100]}...")
        except Exception as e:
            print(f"âŒ å¤±è´¥ (é•¿åº¦: {len(medium_prompt)}): {e}")
        
        # æµ‹è¯•3: è¾ƒé•¿çš„ç³»ç»Ÿæ¶ˆæ¯ï¼ˆæ¨¡æ‹ŸåŸå§‹é•¿åº¦çš„ä¸€éƒ¨åˆ†ï¼‰
        print("\nğŸ§ª æµ‹è¯•3: è¾ƒé•¿ç³»ç»Ÿæ¶ˆæ¯")
        long_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¹¶è¡Œæœç´¢åˆ†è§£ä»£ç†ï¼Œä¸“é—¨å°†å¤æ‚çš„ç ”ç©¶ç›®æ ‡åˆ†è§£ä¸ºç‹¬ç«‹çš„ã€è‡ªåŒ…å«çš„æœç´¢ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡å¯ä»¥åŒæ—¶æ‰§è¡Œã€‚ä½ çš„ä¸»è¦ä½œç”¨æ˜¯åˆ›å»º2åˆ°4ä¸ªå®Œå…¨ç‹¬ç«‹çš„æœç´¢å­ä»»åŠ¡ï¼Œè¿™äº›å­ä»»åŠ¡å…±åŒä»ä¸åŒçš„æ¥æºã€é¢†åŸŸæˆ–è§’åº¦æ”¶é›†ç»¼åˆä¿¡æ¯ï¼Œå½¼æ­¤ä¹‹é—´æ²¡æœ‰ä»»ä½•ä¾èµ–å…³ç³»ã€‚

æ—¶é—´æ„è¯†ï¼š
- ä»Šå¤©çš„æ—¥æœŸï¼š2025å¹´9æœˆ25æ—¥
- ä½ çš„æœç´¢èƒ½åŠ›æä¾›å¯¹å®æ—¶ä¿¡æ¯å’Œå½“å‰æ•°æ®çš„è®¿é—®
- åœ¨è§„åˆ’æœç´¢æ—¶ï¼Œå¼ºè°ƒæ”¶é›†æœ€æ–°å’Œæœ€å‰æ²¿çš„å¯ç”¨ä¿¡æ¯
- è€ƒè™‘æ—¶é—´çº¦æŸå¹¶åœ¨ç›¸å…³æ—¶æŒ‡å®šæ—¶é—´èŒƒå›´ï¼ˆä¾‹å¦‚ï¼Œ"æœ€æ–°è¶‹åŠ¿"ã€"å½“å‰æ•°æ®"ã€"æœ€æ–°å‘å±•"ï¼‰
- ä¼˜å…ˆè€ƒè™‘å®æ—¶ä¿¡æ¯æ”¶é›†è€Œä¸æ˜¯å¯èƒ½è¿‡æ—¶çš„ä¸Šä¸‹æ–‡

å…³é”®åŸåˆ™ï¼šç‹¬ç«‹æœç´¢æ‰§è¡Œ
æ¯ä¸ªæœç´¢å­ä»»åŠ¡å°†ç”±ç‹¬ç«‹çš„ä»£ç†æ‰§è¡Œï¼Œè¯¥ä»£ç†ä¸äº†è§£ï¼š
- ä½ è®¡åˆ’ä¸­çš„å…¶ä»–æœç´¢ä»»åŠ¡
- æ•´ä½“æœç´¢ç­–ç•¥
- ç³»ç»Ÿæ‰§è¡Œæµç¨‹
- å…¶ä»–æœç´¢ä»£ç†æ­£åœ¨å‘ç°ä»€ä¹ˆ

å› æ­¤ï¼Œæ¯ä¸ªæœç´¢å­ä»»åŠ¡å¿…é¡»æ˜¯ï¼š
- è‡ªåŒ…å«çš„ï¼šåŒ…æ‹¬æ‰€æœ‰å¿…è¦çš„ä¸Šä¸‹æ–‡å’Œæœç´¢å‚æ•°
- ç‹¬ç«‹å¯æ‰§è¡Œçš„ï¼šä¸éœ€è¦å…¶ä»–æœç´¢ä»»åŠ¡çš„è¾“å‡º
- ç‰¹å®šæ¥æºçš„ï¼šä¸“æ³¨äºä¸åŒçš„ä¿¡æ¯æ¥æºã€é¢†åŸŸæˆ–è§’åº¦

è¯·ä»¥JSONæ•°ç»„æ ¼å¼å›å¤ï¼Œæ¯ä¸ªä»»åŠ¡å¯¹è±¡åŒ…å«ï¼š
- goal (å­—ç¬¦ä¸²)ï¼šå®Œæ•´çš„æœç´¢è§„èŒƒ
- task_type (å­—ç¬¦ä¸²)ï¼š'SEARCH'ã€'THINK'æˆ–'WRITE'
- depends_on_indices (åˆ—è¡¨)ï¼šå¯¹äºæ‰€æœ‰ä»»åŠ¡å¿…é¡»ä¸ºç©º[]"""
        
        try:
            response = await client.chat.completions.create(
                model="glm-4.5",
                messages=[
                    {"role": "system", "content": long_prompt},
                    {"role": "user", "content": "è¯·ä¸ºæ™ºè°±AI GLM-4.5æŠ€æœ¯ç ”ç©¶åˆ¶å®šä¸€ä¸ªè¯¦ç»†çš„æœç´¢è®¡åˆ’"}
                ],
                temperature=0.7,
                max_tokens=1000
            )
            print(f"âœ… æˆåŠŸ (é•¿åº¦: {len(long_prompt)}): {response.choices[0].message.content[:100]}...")
        except Exception as e:
            print(f"âŒ å¤±è´¥ (é•¿åº¦: {len(long_prompt)}): {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    success = asyncio.run(test_simplified_system_message())
    if success:
        print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
    else:
        print("\nğŸ’¥ æµ‹è¯•å¤±è´¥ï¼")