#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GLM-4.5 æ¨¡å‹é›†æˆæµ‹è¯•è„šæœ¬

æµ‹è¯•æ™ºè°±AI GLM-4.5æ¨¡å‹çš„ç›´æ¥APIè°ƒç”¨é›†æˆ
åŒ…æ‹¬æµå¼å’Œéæµå¼å“åº”æµ‹è¯•
"""

import os
import json
import asyncio
import requests
from pathlib import Path
from typing import Dict, Any, Optional
from dotenv import load_dotenv

# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent
os.chdir(project_root)

# åŠ è½½ç¯å¢ƒå˜é‡
env_path = project_root / ".env"
if env_path.exists():
    load_dotenv(env_path)
    print(f"âœ… å·²åŠ è½½ç¯å¢ƒé…ç½®æ–‡ä»¶: {env_path.absolute()}")
else:
    print(f"âš ï¸  ç¯å¢ƒé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {env_path.absolute()}")

def check_zhipuai_key() -> str:
    """æ£€æŸ¥æ™ºè°±AI APIå¯†é’¥"""
    api_key = os.getenv("ZHIPUAI_API_KEY", "")
    if api_key and api_key != "your_zhipuai_key_here":
        print(f"âœ… æ™ºè°±AI APIå¯†é’¥å·²é…ç½®: {api_key[:10]}...")
        return api_key
    else:
        print("âŒ æ™ºè°±AI APIå¯†é’±æœªé…ç½®æˆ–ä½¿ç”¨é»˜è®¤å€¼")
        return ""

def test_direct_api_call(api_key: str, stream: bool = False) -> bool:
    """æµ‹è¯•ç›´æ¥APIè°ƒç”¨"""
    print(f"\nğŸ§ª æµ‹è¯•GLM-4.5ç›´æ¥APIè°ƒç”¨ (stream={stream})...")
    
    url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    
    data = {
        "model": "glm-4.5",
        "messages": [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"
            },
            {
                "role": "user",
                "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚"
            }
        ],
        "temperature": 0.6,
        "stream": stream
    }
    
    try:
        response = requests.post(url, headers=headers, json=data, timeout=30)
        response.raise_for_status()
        
        if stream:
            # å¤„ç†æµå¼å“åº”
            print("ğŸ“¡ æµå¼å“åº”:")
            for line in response.iter_lines():
                if line:
                    line_text = line.decode('utf-8')
                    if line_text.startswith("data: "):
                        data_part = line_text[6:]
                        if data_part.strip() != "[DONE]":
                            try:
                                json_data = json.loads(data_part)
                                if "choices" in json_data and json_data["choices"]:
                                    delta = json_data["choices"][0].get("delta", {})
                                    if "content" in delta:
                                        print(delta["content"], end="", flush=True)
                            except json.JSONDecodeError:
                                continue
            print("\n")
        else:
            # å¤„ç†éæµå¼å“åº”
            result = response.json()
            print("ğŸ“„ éæµå¼å“åº”:")
            if "choices" in result and result["choices"]:
                content = result["choices"][0]["message"]["content"]
                print(f"å›å¤: {content}")
            else:
                print(f"åŸå§‹å“åº”: {result}")
        
        print(f"âœ… ç›´æ¥APIè°ƒç”¨æˆåŠŸ (stream={stream})")
        return True
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ ç›´æ¥APIè°ƒç”¨å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âŒ å¤„ç†å“åº”å¤±è´¥: {e}")
        return False

def test_litellm_integration(api_key: str) -> bool:
    """æµ‹è¯•LiteLLMé›†æˆ"""
    print(f"\nğŸ”§ æµ‹è¯•LiteLLMé›†æˆGLM-4.5...")
    
    try:
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ["ZHIPUAI_API_KEY"] = api_key
        
        # å¯¼å…¥LiteLLM
        import litellm
        
        # é…ç½®LiteLLM for æ™ºè°±AI
        litellm.api_base = "https://open.bigmodel.cn/api/paas/v4/"
        
        # æµ‹è¯•completionè°ƒç”¨
        response = litellm.completion(
            model="zhipuai/glm-4.5",
            messages=[
                {
                    "role": "system", 
                    "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"
                },
                {
                    "role": "user", 
                    "content": "è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹ä½ çš„èƒ½åŠ›ã€‚"
                }
            ],
            temperature=0.6,
            api_key=api_key
        )
        
        try:
            if response and hasattr(response, 'choices') and response.choices:
                content = getattr(response.choices[0], 'message', None)
                if content and hasattr(content, 'content'):
                    print(f"ğŸ¤– LiteLLMå“åº”: {content.content}")
                    print("âœ… LiteLLMé›†æˆæˆåŠŸ")
                    return True
            print("âŒ LiteLLMå“åº”ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®")
            return False
        except AttributeError as e:
            print(f"âŒ LiteLLMå“åº”å±æ€§é”™è¯¯: {e}")
            return False
            
    except ImportError:
        print("âŒ LiteLLMæœªå®‰è£…ï¼Œè·³è¿‡é›†æˆæµ‹è¯•")
        return False
    except Exception as e:
        print(f"âŒ LiteLLMé›†æˆå¤±è´¥: {e}")
        return False

def test_model_config_validation() -> bool:
    """æµ‹è¯•æ¨¡å‹é…ç½®éªŒè¯"""
    print(f"\nâš™ï¸  æµ‹è¯•æ¨¡å‹é…ç½®éªŒè¯...")
    
    try:
        # å¯¼å…¥æ¨¡å‹é…ç½®ç±»
        import sys
        sys.path.append(str(project_root / "src"))
        from sentientresearchagent.hierarchical_agent_framework.agent_configs.models import ModelConfig
        
        # æµ‹è¯•é…ç½®1: æ™ºè°±AIé€šè¿‡LiteLLM
        config1_dict = {
            "provider": "litellm",
            "model_id": "zhipuai/glm-4.5",
            "temperature": 0.6,
            "max_tokens": 2000
        }
        
        config1 = ModelConfig(**config1_dict)
        print(f"âœ… é…ç½®éªŒè¯æˆåŠŸ: {config1.provider}/{config1.model_id}")
        
        # æµ‹è¯•é…ç½®2: ç›´æ¥æ™ºè°±AI provider
        config2_dict = {
            "provider": "zhipuai",
            "model_id": "glm-4.5",
            "temperature": 0.7
        }
        
        config2 = ModelConfig(**config2_dict)
        print(f"âœ… é…ç½®éªŒè¯æˆåŠŸ: {config2.provider}/{config2.model_id}")
        
        return True
        
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥æ¨¡å‹é…ç½®ç±»: {e}")
        return False
    except Exception as e:
        print(f"âŒ æ¨¡å‹é…ç½®éªŒè¯å¤±è´¥: {e}")
        return False

def test_agent_factory_integration() -> bool:
    """æµ‹è¯•AgentFactoryé›†æˆ"""
    print(f"\nğŸ­ æµ‹è¯•AgentFactoryé›†æˆ...")
    
    try:
        # å¯¼å…¥å¿…è¦çš„ç±»
        import sys
        sys.path.append(str(project_root / "src"))
        from sentientresearchagent.hierarchical_agent_framework.agent_configs.agent_factory import AgentFactory
        from sentientresearchagent.hierarchical_agent_framework.agent_configs.models import ModelConfig
        
        # åˆ›å»ºæ¨¡å‹é…ç½®
        model_config = ModelConfig(
            provider="zhipuai",
            model_id="glm-4.5",
            temperature=0.6
        )
        
        # åˆ›å»ºAgentFactoryå®ä¾‹ (éœ€è¦config_loader, è¿™é‡Œæ¨¡æ‹Ÿä¸€ä¸ª)
        class MockConfigLoader:
            def resolve_prompt(self, prompt_source):
                return "You are a helpful AI assistant."
        
        factory = AgentFactory(MockConfigLoader())
        
        # æµ‹è¯•æ¨¡å‹åˆ›å»º
        try:
            model_instance = factory.create_model_instance(model_config)
            print(f"âœ… AgentFactoryæ¨¡å‹åˆ›å»ºæˆåŠŸ: {type(model_instance).__name__}")
            return True
        except Exception as e:
            print(f"âŒ AgentFactoryæ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            return False
            
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥AgentFactoryç±»: {e}")
        return False
    except Exception as e:
        print(f"âŒ AgentFactoryé›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False

def create_sample_agent_config() -> bool:
    """åˆ›å»ºç¤ºä¾‹æ™ºè°±AIä»£ç†é…ç½®"""
    print(f"\nğŸ“ åˆ›å»ºç¤ºä¾‹GLM-4.5ä»£ç†é…ç½®...")
    
    sample_config = {
        "agents": [
            {
                "name": "glm45-planner",
                "type": "planner",
                "adapter_class": "PlannerAdapter",
                "description": "ä½¿ç”¨GLM-4.5çš„è§„åˆ’ä»£ç†",
                "enabled": True,
                "model": {
                    "provider": "zhipuai",
                    "model_id": "glm-4.5",
                    "temperature": 0.6,
                    "max_tokens": 2000
                },
                "prompt_source": "prompts.planner_prompts.SYSTEM_MESSAGE",
                "response_model": "PlanOutput",
                "tools": [],
                "toolkits": []
            },
            {
                "name": "glm45-executor",
                "type": "executor", 
                "adapter_class": "ExecutorAdapter",
                "description": "ä½¿ç”¨GLM-4.5çš„æ‰§è¡Œä»£ç†",
                "enabled": True,
                "model": {
                    "provider": "litellm",
                    "model_id": "zhipuai/glm-4.5",
                    "temperature": 0.7,
                    "max_tokens": 1500
                },
                "prompt_source": "prompts.executor_prompts.SYSTEM_MESSAGE",
                "tools": ["web_search"],
                "toolkits": []
            }
        ],
        "metadata": {
            "created_for": "GLM-4.5é›†æˆæµ‹è¯•",
            "api_version": "v4",
            "model_version": "glm-4.5"
        }
    }
    
    try:
        config_path = project_root / "examples" / "glm45-agents-config.yaml"
        config_path.parent.mkdir(exist_ok=True)
        
        import yaml
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(sample_config, f, default_flow_style=False, allow_unicode=True, indent=2)
        
        print(f"âœ… ç¤ºä¾‹é…ç½®å·²åˆ›å»º: {config_path}")
        return True
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºç¤ºä¾‹é…ç½®å¤±è´¥: {e}")
        return False

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ GLM-4.5æ¨¡å‹é›†æˆæµ‹è¯•å¼€å§‹...")
    
    # æ£€æŸ¥APIå¯†é’¥
    api_key = check_zhipuai_key()
    
    test_results = []
    
    # æµ‹è¯•1: æ¨¡å‹é…ç½®éªŒè¯
    result1 = test_model_config_validation()
    test_results.append(("æ¨¡å‹é…ç½®éªŒè¯", result1))
    
    # æµ‹è¯•2: åˆ›å»ºç¤ºä¾‹é…ç½®
    result2 = create_sample_agent_config()
    test_results.append(("ç¤ºä¾‹é…ç½®åˆ›å»º", result2))
    
    # å¦‚æœæœ‰APIå¯†é’¥ï¼Œè¿›è¡ŒAPIè°ƒç”¨æµ‹è¯•
    if api_key:
        # æµ‹è¯•3: ç›´æ¥APIè°ƒç”¨ (éæµå¼)
        result3 = test_direct_api_call(api_key, stream=False)
        test_results.append(("ç›´æ¥APIè°ƒç”¨(éæµå¼)", result3))
        
        # æµ‹è¯•4: ç›´æ¥APIè°ƒç”¨ (æµå¼)
        result4 = test_direct_api_call(api_key, stream=True)
        test_results.append(("ç›´æ¥APIè°ƒç”¨(æµå¼)", result4))
        
        # æµ‹è¯•5: LiteLLMé›†æˆ
        result5 = test_litellm_integration(api_key)
        test_results.append(("LiteLLMé›†æˆ", result5))
    else:
        print("\nâš ï¸  è·³è¿‡APIè°ƒç”¨æµ‹è¯• (éœ€è¦æœ‰æ•ˆçš„ZHIPUAI_API_KEY)")
    
    # æµ‹è¯•6: AgentFactoryé›†æˆ
    result6 = test_agent_factory_integration()
    test_results.append(("AgentFactoryé›†æˆ", result6))
    
    # æ±‡æ€»ç»“æœ
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
    print("-" * 50)
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
        if result:
            passed += 1
    
    print("-" * 50)
    print(f"é€šè¿‡ç‡: {passed}/{total} ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼GLM-4.5é›†æˆæˆåŠŸï¼")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œä¾èµ–ã€‚")

if __name__ == "__main__":
    asyncio.run(main())